{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Convert a PyTorch model to Tensorflow using ONNX\n",
    "\n",
    "In this tutorial, we will show you how to export a model defined in PyTorch to ONNX and then import the ONNX model into Tensorflow to run it. We will also show you how to save this Tensorflow model into a file for later use.  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Installations\n",
    "\n",
    "First let's install [ONNX](https://github.com/onnx/onnx), [PyTorch](https://github.com/pytorch/pytorch), and [Tensorflow](https://github.com/tensorflow/tensorflow) by following the instructions on each of their repository.\n",
    "\n",
    "Then Install torchvision by the following command:\n",
    "```\n",
    "pip install torchvision\n",
    "```\n",
    "\n",
    "Next install [onnx-tensorflow](https://github.com/onnx/onnx-tensorflow) by the following commands:\n",
    "```\n",
    "git clone git@github.com:onnx/onnx-tensorflow.git && cd onnx-tensorflow\n",
    "pip install -e .\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define model\n",
    "\n",
    "In this tutorial we are going to use the [MNIST model](https://github.com/pytorch/examples/tree/master/mnist) from PyTorch examples.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "\n",
    "class Net(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Net, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(1, 10, kernel_size=5)\n",
    "        self.conv2 = nn.Conv2d(10, 20, kernel_size=5)\n",
    "        self.conv2_drop = nn.Dropout2d()\n",
    "        self.fc1 = nn.Linear(320, 50)\n",
    "        self.fc2 = nn.Linear(50, 10)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = F.relu(F.max_pool2d(self.conv1(x), 2))\n",
    "        x = F.relu(F.max_pool2d(self.conv2_drop(self.conv2(x)), 2))\n",
    "        x = x.view(-1, 320)\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = F.dropout(x, training=self.training)\n",
    "        x = self.fc2(x)\n",
    "        return F.log_softmax(x, dim=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train and test model\n",
    "Now let's train this model. By default if GPU is availalbe on your environment, it will use GPU instead of CPU to run the training. In this tutorial we will train this model with 60 epochs. It will takes about 15 minutes on an environment with 1 GPU to complete this training. You can always adjust the number of epoch base on how well you want your model to be trained."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Epoch: 1 [0/60000 (0%)]\tLoss: 2.351818\n",
      "Train Epoch: 1 [19200/60000 (32%)]\tLoss: 0.859197\n",
      "Train Epoch: 1 [38400/60000 (64%)]\tLoss: 0.659584\n",
      "Train Epoch: 1 [57600/60000 (96%)]\tLoss: 0.340543\n",
      "\n",
      "Test set: Average loss: 0.2000, Accuracy: 9432/10000 (94%)\n",
      "\n",
      "Train Epoch: 2 [0/60000 (0%)]\tLoss: 0.481282\n",
      "Train Epoch: 2 [19200/60000 (32%)]\tLoss: 0.362192\n",
      "Train Epoch: 2 [38400/60000 (64%)]\tLoss: 0.307075\n",
      "Train Epoch: 2 [57600/60000 (96%)]\tLoss: 0.529445\n",
      "\n",
      "Test set: Average loss: 0.1202, Accuracy: 9621/10000 (96%)\n",
      "\n",
      "Train Epoch: 3 [0/60000 (0%)]\tLoss: 0.423853\n",
      "Train Epoch: 3 [19200/60000 (32%)]\tLoss: 0.416131\n",
      "Train Epoch: 3 [38400/60000 (64%)]\tLoss: 0.295029\n",
      "Train Epoch: 3 [57600/60000 (96%)]\tLoss: 0.377283\n",
      "\n",
      "Test set: Average loss: 0.0985, Accuracy: 9686/10000 (97%)\n",
      "\n",
      "Train Epoch: 4 [0/60000 (0%)]\tLoss: 0.383130\n",
      "Train Epoch: 4 [19200/60000 (32%)]\tLoss: 0.169997\n",
      "Train Epoch: 4 [38400/60000 (64%)]\tLoss: 0.115847\n",
      "Train Epoch: 4 [57600/60000 (96%)]\tLoss: 0.217762\n",
      "\n",
      "Test set: Average loss: 0.0859, Accuracy: 9739/10000 (97%)\n",
      "\n",
      "Train Epoch: 5 [0/60000 (0%)]\tLoss: 0.390293\n",
      "Train Epoch: 5 [19200/60000 (32%)]\tLoss: 0.413035\n",
      "Train Epoch: 5 [38400/60000 (64%)]\tLoss: 0.111475\n",
      "Train Epoch: 5 [57600/60000 (96%)]\tLoss: 0.311389\n",
      "\n",
      "Test set: Average loss: 0.0758, Accuracy: 9746/10000 (97%)\n",
      "\n",
      "Train Epoch: 6 [0/60000 (0%)]\tLoss: 0.172327\n",
      "Train Epoch: 6 [19200/60000 (32%)]\tLoss: 0.237009\n",
      "Train Epoch: 6 [38400/60000 (64%)]\tLoss: 0.336602\n",
      "Train Epoch: 6 [57600/60000 (96%)]\tLoss: 0.099094\n",
      "\n",
      "Test set: Average loss: 0.0666, Accuracy: 9777/10000 (98%)\n",
      "\n",
      "Train Epoch: 7 [0/60000 (0%)]\tLoss: 0.146474\n",
      "Train Epoch: 7 [19200/60000 (32%)]\tLoss: 0.251859\n",
      "Train Epoch: 7 [38400/60000 (64%)]\tLoss: 0.099362\n",
      "Train Epoch: 7 [57600/60000 (96%)]\tLoss: 0.259238\n",
      "\n",
      "Test set: Average loss: 0.0613, Accuracy: 9809/10000 (98%)\n",
      "\n",
      "Train Epoch: 8 [0/60000 (0%)]\tLoss: 0.199104\n",
      "Train Epoch: 8 [19200/60000 (32%)]\tLoss: 0.289850\n",
      "Train Epoch: 8 [38400/60000 (64%)]\tLoss: 0.105517\n",
      "Train Epoch: 8 [57600/60000 (96%)]\tLoss: 0.130536\n",
      "\n",
      "Test set: Average loss: 0.0565, Accuracy: 9834/10000 (98%)\n",
      "\n",
      "Train Epoch: 9 [0/60000 (0%)]\tLoss: 0.166462\n",
      "Train Epoch: 9 [19200/60000 (32%)]\tLoss: 0.350009\n",
      "Train Epoch: 9 [38400/60000 (64%)]\tLoss: 0.116394\n",
      "Train Epoch: 9 [57600/60000 (96%)]\tLoss: 0.330358\n",
      "\n",
      "Test set: Average loss: 0.0531, Accuracy: 9830/10000 (98%)\n",
      "\n",
      "Train Epoch: 10 [0/60000 (0%)]\tLoss: 0.158911\n",
      "Train Epoch: 10 [19200/60000 (32%)]\tLoss: 0.077070\n",
      "Train Epoch: 10 [38400/60000 (64%)]\tLoss: 0.118338\n",
      "Train Epoch: 10 [57600/60000 (96%)]\tLoss: 0.216225\n",
      "\n",
      "Test set: Average loss: 0.0524, Accuracy: 9842/10000 (98%)\n",
      "\n",
      "Train Epoch: 11 [0/60000 (0%)]\tLoss: 0.112984\n",
      "Train Epoch: 11 [19200/60000 (32%)]\tLoss: 0.138594\n",
      "Train Epoch: 11 [38400/60000 (64%)]\tLoss: 0.201079\n",
      "Train Epoch: 11 [57600/60000 (96%)]\tLoss: 0.210765\n",
      "\n",
      "Test set: Average loss: 0.0471, Accuracy: 9852/10000 (99%)\n",
      "\n",
      "Train Epoch: 12 [0/60000 (0%)]\tLoss: 0.169629\n",
      "Train Epoch: 12 [19200/60000 (32%)]\tLoss: 0.058720\n",
      "Train Epoch: 12 [38400/60000 (64%)]\tLoss: 0.148005\n",
      "Train Epoch: 12 [57600/60000 (96%)]\tLoss: 0.072339\n",
      "\n",
      "Test set: Average loss: 0.0464, Accuracy: 9854/10000 (99%)\n",
      "\n",
      "Train Epoch: 13 [0/60000 (0%)]\tLoss: 0.086229\n",
      "Train Epoch: 13 [19200/60000 (32%)]\tLoss: 0.085300\n",
      "Train Epoch: 13 [38400/60000 (64%)]\tLoss: 0.099229\n",
      "Train Epoch: 13 [57600/60000 (96%)]\tLoss: 0.149542\n",
      "\n",
      "Test set: Average loss: 0.0441, Accuracy: 9862/10000 (99%)\n",
      "\n",
      "Train Epoch: 14 [0/60000 (0%)]\tLoss: 0.062021\n",
      "Train Epoch: 14 [19200/60000 (32%)]\tLoss: 0.109803\n",
      "Train Epoch: 14 [38400/60000 (64%)]\tLoss: 0.042602\n",
      "Train Epoch: 14 [57600/60000 (96%)]\tLoss: 0.172992\n",
      "\n",
      "Test set: Average loss: 0.0412, Accuracy: 9870/10000 (99%)\n",
      "\n",
      "Train Epoch: 15 [0/60000 (0%)]\tLoss: 0.117488\n",
      "Train Epoch: 15 [19200/60000 (32%)]\tLoss: 0.219923\n",
      "Train Epoch: 15 [38400/60000 (64%)]\tLoss: 0.234651\n",
      "Train Epoch: 15 [57600/60000 (96%)]\tLoss: 0.114379\n",
      "\n",
      "Test set: Average loss: 0.0426, Accuracy: 9877/10000 (99%)\n",
      "\n",
      "Train Epoch: 16 [0/60000 (0%)]\tLoss: 0.153571\n",
      "Train Epoch: 16 [19200/60000 (32%)]\tLoss: 0.037048\n",
      "Train Epoch: 16 [38400/60000 (64%)]\tLoss: 0.185502\n",
      "Train Epoch: 16 [57600/60000 (96%)]\tLoss: 0.049746\n",
      "\n",
      "Test set: Average loss: 0.0427, Accuracy: 9871/10000 (99%)\n",
      "\n",
      "Train Epoch: 17 [0/60000 (0%)]\tLoss: 0.094805\n",
      "Train Epoch: 17 [19200/60000 (32%)]\tLoss: 0.133581\n",
      "Train Epoch: 17 [38400/60000 (64%)]\tLoss: 0.185188\n",
      "Train Epoch: 17 [57600/60000 (96%)]\tLoss: 0.218906\n",
      "\n",
      "Test set: Average loss: 0.0397, Accuracy: 9877/10000 (99%)\n",
      "\n",
      "Train Epoch: 18 [0/60000 (0%)]\tLoss: 0.071954\n",
      "Train Epoch: 18 [19200/60000 (32%)]\tLoss: 0.078671\n",
      "Train Epoch: 18 [38400/60000 (64%)]\tLoss: 0.130887\n",
      "Train Epoch: 18 [57600/60000 (96%)]\tLoss: 0.198495\n",
      "\n",
      "Test set: Average loss: 0.0388, Accuracy: 9883/10000 (99%)\n",
      "\n",
      "Train Epoch: 19 [0/60000 (0%)]\tLoss: 0.151028\n",
      "Train Epoch: 19 [19200/60000 (32%)]\tLoss: 0.172894\n",
      "Train Epoch: 19 [38400/60000 (64%)]\tLoss: 0.184779\n",
      "Train Epoch: 19 [57600/60000 (96%)]\tLoss: 0.130046\n",
      "\n",
      "Test set: Average loss: 0.0406, Accuracy: 9873/10000 (99%)\n",
      "\n",
      "Train Epoch: 20 [0/60000 (0%)]\tLoss: 0.147692\n",
      "Train Epoch: 20 [19200/60000 (32%)]\tLoss: 0.339226\n",
      "Train Epoch: 20 [38400/60000 (64%)]\tLoss: 0.085207\n",
      "Train Epoch: 20 [57600/60000 (96%)]\tLoss: 0.063149\n",
      "\n",
      "Test set: Average loss: 0.0403, Accuracy: 9868/10000 (99%)\n",
      "\n",
      "Train Epoch: 21 [0/60000 (0%)]\tLoss: 0.101928\n",
      "Train Epoch: 21 [19200/60000 (32%)]\tLoss: 0.049627\n",
      "Train Epoch: 21 [38400/60000 (64%)]\tLoss: 0.070329\n",
      "Train Epoch: 21 [57600/60000 (96%)]\tLoss: 0.194384\n",
      "\n",
      "Test set: Average loss: 0.0391, Accuracy: 9873/10000 (99%)\n",
      "\n",
      "Train Epoch: 22 [0/60000 (0%)]\tLoss: 0.039554\n",
      "Train Epoch: 22 [19200/60000 (32%)]\tLoss: 0.061052\n",
      "Train Epoch: 22 [38400/60000 (64%)]\tLoss: 0.088321\n",
      "Train Epoch: 22 [57600/60000 (96%)]\tLoss: 0.222511\n",
      "\n",
      "Test set: Average loss: 0.0381, Accuracy: 9880/10000 (99%)\n",
      "\n",
      "Train Epoch: 23 [0/60000 (0%)]\tLoss: 0.093077\n",
      "Train Epoch: 23 [19200/60000 (32%)]\tLoss: 0.136186\n",
      "Train Epoch: 23 [38400/60000 (64%)]\tLoss: 0.105003\n",
      "Train Epoch: 23 [57600/60000 (96%)]\tLoss: 0.248202\n",
      "\n",
      "Test set: Average loss: 0.0359, Accuracy: 9881/10000 (99%)\n",
      "\n",
      "Train Epoch: 24 [0/60000 (0%)]\tLoss: 0.097371\n",
      "Train Epoch: 24 [19200/60000 (32%)]\tLoss: 0.120352\n",
      "Train Epoch: 24 [38400/60000 (64%)]\tLoss: 0.066454\n",
      "Train Epoch: 24 [57600/60000 (96%)]\tLoss: 0.195088\n",
      "\n",
      "Test set: Average loss: 0.0375, Accuracy: 9882/10000 (99%)\n",
      "\n",
      "Train Epoch: 25 [0/60000 (0%)]\tLoss: 0.203047\n",
      "Train Epoch: 25 [19200/60000 (32%)]\tLoss: 0.062469\n",
      "Train Epoch: 25 [38400/60000 (64%)]\tLoss: 0.126101\n",
      "Train Epoch: 25 [57600/60000 (96%)]\tLoss: 0.045966\n",
      "\n",
      "Test set: Average loss: 0.0365, Accuracy: 9887/10000 (99%)\n",
      "\n",
      "Train Epoch: 26 [0/60000 (0%)]\tLoss: 0.150108\n",
      "Train Epoch: 26 [19200/60000 (32%)]\tLoss: 0.106967\n",
      "Train Epoch: 26 [38400/60000 (64%)]\tLoss: 0.152033\n",
      "Train Epoch: 26 [57600/60000 (96%)]\tLoss: 0.136922\n",
      "\n",
      "Test set: Average loss: 0.0351, Accuracy: 9889/10000 (99%)\n",
      "\n",
      "Train Epoch: 27 [0/60000 (0%)]\tLoss: 0.150505\n",
      "Train Epoch: 27 [19200/60000 (32%)]\tLoss: 0.151195\n",
      "Train Epoch: 27 [38400/60000 (64%)]\tLoss: 0.139996\n",
      "Train Epoch: 27 [57600/60000 (96%)]\tLoss: 0.099971\n",
      "\n",
      "Test set: Average loss: 0.0341, Accuracy: 9892/10000 (99%)\n",
      "\n",
      "Train Epoch: 28 [0/60000 (0%)]\tLoss: 0.058916\n",
      "Train Epoch: 28 [19200/60000 (32%)]\tLoss: 0.272777\n",
      "Train Epoch: 28 [38400/60000 (64%)]\tLoss: 0.186844\n",
      "Train Epoch: 28 [57600/60000 (96%)]\tLoss: 0.058663\n",
      "\n",
      "Test set: Average loss: 0.0359, Accuracy: 9894/10000 (99%)\n",
      "\n",
      "Train Epoch: 29 [0/60000 (0%)]\tLoss: 0.027084\n",
      "Train Epoch: 29 [19200/60000 (32%)]\tLoss: 0.111287\n",
      "Train Epoch: 29 [38400/60000 (64%)]\tLoss: 0.030373\n",
      "Train Epoch: 29 [57600/60000 (96%)]\tLoss: 0.064727\n",
      "\n",
      "Test set: Average loss: 0.0336, Accuracy: 9898/10000 (99%)\n",
      "\n",
      "Train Epoch: 30 [0/60000 (0%)]\tLoss: 0.074574\n",
      "Train Epoch: 30 [19200/60000 (32%)]\tLoss: 0.198754\n",
      "Train Epoch: 30 [38400/60000 (64%)]\tLoss: 0.127824\n",
      "Train Epoch: 30 [57600/60000 (96%)]\tLoss: 0.084175\n",
      "\n",
      "Test set: Average loss: 0.0337, Accuracy: 9893/10000 (99%)\n",
      "\n",
      "Train Epoch: 31 [0/60000 (0%)]\tLoss: 0.180867\n",
      "Train Epoch: 31 [19200/60000 (32%)]\tLoss: 0.064226\n",
      "Train Epoch: 31 [38400/60000 (64%)]\tLoss: 0.085359\n",
      "Train Epoch: 31 [57600/60000 (96%)]\tLoss: 0.238225\n",
      "\n",
      "Test set: Average loss: 0.0327, Accuracy: 9899/10000 (99%)\n",
      "\n",
      "Train Epoch: 32 [0/60000 (0%)]\tLoss: 0.105831\n",
      "Train Epoch: 32 [19200/60000 (32%)]\tLoss: 0.052196\n",
      "Train Epoch: 32 [38400/60000 (64%)]\tLoss: 0.175171\n",
      "Train Epoch: 32 [57600/60000 (96%)]\tLoss: 0.030759\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Test set: Average loss: 0.0316, Accuracy: 9891/10000 (99%)\n",
      "\n",
      "Train Epoch: 33 [0/60000 (0%)]\tLoss: 0.124312\n",
      "Train Epoch: 33 [19200/60000 (32%)]\tLoss: 0.266243\n",
      "Train Epoch: 33 [38400/60000 (64%)]\tLoss: 0.164875\n",
      "Train Epoch: 33 [57600/60000 (96%)]\tLoss: 0.184419\n",
      "\n",
      "Test set: Average loss: 0.0325, Accuracy: 9901/10000 (99%)\n",
      "\n",
      "Train Epoch: 34 [0/60000 (0%)]\tLoss: 0.102198\n",
      "Train Epoch: 34 [19200/60000 (32%)]\tLoss: 0.093716\n",
      "Train Epoch: 34 [38400/60000 (64%)]\tLoss: 0.180865\n",
      "Train Epoch: 34 [57600/60000 (96%)]\tLoss: 0.045589\n",
      "\n",
      "Test set: Average loss: 0.0320, Accuracy: 9899/10000 (99%)\n",
      "\n",
      "Train Epoch: 35 [0/60000 (0%)]\tLoss: 0.134286\n",
      "Train Epoch: 35 [19200/60000 (32%)]\tLoss: 0.182304\n",
      "Train Epoch: 35 [38400/60000 (64%)]\tLoss: 0.218468\n",
      "Train Epoch: 35 [57600/60000 (96%)]\tLoss: 0.128319\n",
      "\n",
      "Test set: Average loss: 0.0336, Accuracy: 9900/10000 (99%)\n",
      "\n",
      "Train Epoch: 36 [0/60000 (0%)]\tLoss: 0.125603\n",
      "Train Epoch: 36 [19200/60000 (32%)]\tLoss: 0.080189\n",
      "Train Epoch: 36 [38400/60000 (64%)]\tLoss: 0.060245\n",
      "Train Epoch: 36 [57600/60000 (96%)]\tLoss: 0.121766\n",
      "\n",
      "Test set: Average loss: 0.0340, Accuracy: 9897/10000 (99%)\n",
      "\n",
      "Train Epoch: 37 [0/60000 (0%)]\tLoss: 0.258007\n",
      "Train Epoch: 37 [19200/60000 (32%)]\tLoss: 0.023958\n",
      "Train Epoch: 37 [38400/60000 (64%)]\tLoss: 0.085051\n",
      "Train Epoch: 37 [57600/60000 (96%)]\tLoss: 0.037916\n",
      "\n",
      "Test set: Average loss: 0.0318, Accuracy: 9894/10000 (99%)\n",
      "\n",
      "Train Epoch: 38 [0/60000 (0%)]\tLoss: 0.063393\n",
      "Train Epoch: 38 [19200/60000 (32%)]\tLoss: 0.108414\n",
      "Train Epoch: 38 [38400/60000 (64%)]\tLoss: 0.045751\n",
      "Train Epoch: 38 [57600/60000 (96%)]\tLoss: 0.058877\n",
      "\n",
      "Test set: Average loss: 0.0304, Accuracy: 9903/10000 (99%)\n",
      "\n",
      "Train Epoch: 39 [0/60000 (0%)]\tLoss: 0.119873\n",
      "Train Epoch: 39 [19200/60000 (32%)]\tLoss: 0.117172\n",
      "Train Epoch: 39 [38400/60000 (64%)]\tLoss: 0.210982\n",
      "Train Epoch: 39 [57600/60000 (96%)]\tLoss: 0.080806\n",
      "\n",
      "Test set: Average loss: 0.0327, Accuracy: 9897/10000 (99%)\n",
      "\n",
      "Train Epoch: 40 [0/60000 (0%)]\tLoss: 0.143257\n",
      "Train Epoch: 40 [19200/60000 (32%)]\tLoss: 0.321466\n",
      "Train Epoch: 40 [38400/60000 (64%)]\tLoss: 0.023734\n",
      "Train Epoch: 40 [57600/60000 (96%)]\tLoss: 0.191103\n",
      "\n",
      "Test set: Average loss: 0.0326, Accuracy: 9900/10000 (99%)\n",
      "\n",
      "Train Epoch: 41 [0/60000 (0%)]\tLoss: 0.171152\n",
      "Train Epoch: 41 [19200/60000 (32%)]\tLoss: 0.105990\n",
      "Train Epoch: 41 [38400/60000 (64%)]\tLoss: 0.075601\n",
      "Train Epoch: 41 [57600/60000 (96%)]\tLoss: 0.070930\n",
      "\n",
      "Test set: Average loss: 0.0315, Accuracy: 9900/10000 (99%)\n",
      "\n",
      "Train Epoch: 42 [0/60000 (0%)]\tLoss: 0.189556\n",
      "Train Epoch: 42 [19200/60000 (32%)]\tLoss: 0.077644\n",
      "Train Epoch: 42 [38400/60000 (64%)]\tLoss: 0.058779\n",
      "Train Epoch: 42 [57600/60000 (96%)]\tLoss: 0.109922\n",
      "\n",
      "Test set: Average loss: 0.0319, Accuracy: 9897/10000 (99%)\n",
      "\n",
      "Train Epoch: 43 [0/60000 (0%)]\tLoss: 0.048801\n",
      "Train Epoch: 43 [19200/60000 (32%)]\tLoss: 0.127942\n",
      "Train Epoch: 43 [38400/60000 (64%)]\tLoss: 0.013618\n",
      "Train Epoch: 43 [57600/60000 (96%)]\tLoss: 0.083227\n",
      "\n",
      "Test set: Average loss: 0.0302, Accuracy: 9908/10000 (99%)\n",
      "\n",
      "Train Epoch: 44 [0/60000 (0%)]\tLoss: 0.140819\n",
      "Train Epoch: 44 [19200/60000 (32%)]\tLoss: 0.245089\n",
      "Train Epoch: 44 [38400/60000 (64%)]\tLoss: 0.080390\n",
      "Train Epoch: 44 [57600/60000 (96%)]\tLoss: 0.198932\n",
      "\n",
      "Test set: Average loss: 0.0323, Accuracy: 9901/10000 (99%)\n",
      "\n",
      "Train Epoch: 45 [0/60000 (0%)]\tLoss: 0.185138\n",
      "Train Epoch: 45 [19200/60000 (32%)]\tLoss: 0.040128\n",
      "Train Epoch: 45 [38400/60000 (64%)]\tLoss: 0.139483\n",
      "Train Epoch: 45 [57600/60000 (96%)]\tLoss: 0.073834\n",
      "\n",
      "Test set: Average loss: 0.0321, Accuracy: 9898/10000 (99%)\n",
      "\n",
      "Train Epoch: 46 [0/60000 (0%)]\tLoss: 0.067727\n",
      "Train Epoch: 46 [19200/60000 (32%)]\tLoss: 0.059875\n",
      "Train Epoch: 46 [38400/60000 (64%)]\tLoss: 0.113601\n",
      "Train Epoch: 46 [57600/60000 (96%)]\tLoss: 0.207854\n",
      "\n",
      "Test set: Average loss: 0.0318, Accuracy: 9900/10000 (99%)\n",
      "\n",
      "Train Epoch: 47 [0/60000 (0%)]\tLoss: 0.024378\n",
      "Train Epoch: 47 [19200/60000 (32%)]\tLoss: 0.182198\n",
      "Train Epoch: 47 [38400/60000 (64%)]\tLoss: 0.168402\n",
      "Train Epoch: 47 [57600/60000 (96%)]\tLoss: 0.146338\n",
      "\n",
      "Test set: Average loss: 0.0321, Accuracy: 9897/10000 (99%)\n",
      "\n",
      "Train Epoch: 48 [0/60000 (0%)]\tLoss: 0.038608\n",
      "Train Epoch: 48 [19200/60000 (32%)]\tLoss: 0.158814\n",
      "Train Epoch: 48 [38400/60000 (64%)]\tLoss: 0.039731\n",
      "Train Epoch: 48 [57600/60000 (96%)]\tLoss: 0.135481\n",
      "\n",
      "Test set: Average loss: 0.0303, Accuracy: 9905/10000 (99%)\n",
      "\n",
      "Train Epoch: 49 [0/60000 (0%)]\tLoss: 0.031294\n",
      "Train Epoch: 49 [19200/60000 (32%)]\tLoss: 0.076328\n",
      "Train Epoch: 49 [38400/60000 (64%)]\tLoss: 0.051792\n",
      "Train Epoch: 49 [57600/60000 (96%)]\tLoss: 0.063955\n",
      "\n",
      "Test set: Average loss: 0.0286, Accuracy: 9902/10000 (99%)\n",
      "\n",
      "Train Epoch: 50 [0/60000 (0%)]\tLoss: 0.148471\n",
      "Train Epoch: 50 [19200/60000 (32%)]\tLoss: 0.129344\n",
      "Train Epoch: 50 [38400/60000 (64%)]\tLoss: 0.142481\n",
      "Train Epoch: 50 [57600/60000 (96%)]\tLoss: 0.267909\n",
      "\n",
      "Test set: Average loss: 0.0317, Accuracy: 9894/10000 (99%)\n",
      "\n",
      "Train Epoch: 51 [0/60000 (0%)]\tLoss: 0.132059\n",
      "Train Epoch: 51 [19200/60000 (32%)]\tLoss: 0.041623\n",
      "Train Epoch: 51 [38400/60000 (64%)]\tLoss: 0.039736\n",
      "Train Epoch: 51 [57600/60000 (96%)]\tLoss: 0.048830\n",
      "\n",
      "Test set: Average loss: 0.0323, Accuracy: 9899/10000 (99%)\n",
      "\n",
      "Train Epoch: 52 [0/60000 (0%)]\tLoss: 0.040338\n",
      "Train Epoch: 52 [19200/60000 (32%)]\tLoss: 0.184282\n",
      "Train Epoch: 52 [38400/60000 (64%)]\tLoss: 0.025192\n",
      "Train Epoch: 52 [57600/60000 (96%)]\tLoss: 0.064288\n",
      "\n",
      "Test set: Average loss: 0.0327, Accuracy: 9900/10000 (99%)\n",
      "\n",
      "Train Epoch: 53 [0/60000 (0%)]\tLoss: 0.079784\n",
      "Train Epoch: 53 [19200/60000 (32%)]\tLoss: 0.087868\n",
      "Train Epoch: 53 [38400/60000 (64%)]\tLoss: 0.118223\n",
      "Train Epoch: 53 [57600/60000 (96%)]\tLoss: 0.036435\n",
      "\n",
      "Test set: Average loss: 0.0354, Accuracy: 9886/10000 (99%)\n",
      "\n",
      "Train Epoch: 54 [0/60000 (0%)]\tLoss: 0.030089\n",
      "Train Epoch: 54 [19200/60000 (32%)]\tLoss: 0.060108\n",
      "Train Epoch: 54 [38400/60000 (64%)]\tLoss: 0.117364\n",
      "Train Epoch: 54 [57600/60000 (96%)]\tLoss: 0.132732\n",
      "\n",
      "Test set: Average loss: 0.0306, Accuracy: 9904/10000 (99%)\n",
      "\n",
      "Train Epoch: 55 [0/60000 (0%)]\tLoss: 0.123055\n",
      "Train Epoch: 55 [19200/60000 (32%)]\tLoss: 0.061948\n",
      "Train Epoch: 55 [38400/60000 (64%)]\tLoss: 0.115145\n",
      "Train Epoch: 55 [57600/60000 (96%)]\tLoss: 0.028329\n",
      "\n",
      "Test set: Average loss: 0.0309, Accuracy: 9901/10000 (99%)\n",
      "\n",
      "Train Epoch: 56 [0/60000 (0%)]\tLoss: 0.088086\n",
      "Train Epoch: 56 [19200/60000 (32%)]\tLoss: 0.120859\n",
      "Train Epoch: 56 [38400/60000 (64%)]\tLoss: 0.044856\n",
      "Train Epoch: 56 [57600/60000 (96%)]\tLoss: 0.205777\n",
      "\n",
      "Test set: Average loss: 0.0309, Accuracy: 9903/10000 (99%)\n",
      "\n",
      "Train Epoch: 57 [0/60000 (0%)]\tLoss: 0.007221\n",
      "Train Epoch: 57 [19200/60000 (32%)]\tLoss: 0.056238\n",
      "Train Epoch: 57 [38400/60000 (64%)]\tLoss: 0.174416\n",
      "Train Epoch: 57 [57600/60000 (96%)]\tLoss: 0.153064\n",
      "\n",
      "Test set: Average loss: 0.0297, Accuracy: 9907/10000 (99%)\n",
      "\n",
      "Train Epoch: 58 [0/60000 (0%)]\tLoss: 0.075164\n",
      "Train Epoch: 58 [19200/60000 (32%)]\tLoss: 0.019936\n",
      "Train Epoch: 58 [38400/60000 (64%)]\tLoss: 0.102272\n",
      "Train Epoch: 58 [57600/60000 (96%)]\tLoss: 0.038017\n",
      "\n",
      "Test set: Average loss: 0.0307, Accuracy: 9904/10000 (99%)\n",
      "\n",
      "Train Epoch: 59 [0/60000 (0%)]\tLoss: 0.061610\n",
      "Train Epoch: 59 [19200/60000 (32%)]\tLoss: 0.299328\n",
      "Train Epoch: 59 [38400/60000 (64%)]\tLoss: 0.108271\n",
      "Train Epoch: 59 [57600/60000 (96%)]\tLoss: 0.091228\n",
      "\n",
      "Test set: Average loss: 0.0307, Accuracy: 9906/10000 (99%)\n",
      "\n",
      "Train Epoch: 60 [0/60000 (0%)]\tLoss: 0.102247\n",
      "Train Epoch: 60 [19200/60000 (32%)]\tLoss: 0.149902\n",
      "Train Epoch: 60 [38400/60000 (64%)]\tLoss: 0.050521\n",
      "Train Epoch: 60 [57600/60000 (96%)]\tLoss: 0.114131\n",
      "\n",
      "Test set: Average loss: 0.0313, Accuracy: 9904/10000 (99%)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import argparse\n",
    "import torch.optim as optim\n",
    "from torchvision import datasets, transforms\n",
    "\n",
    "def train(args, model, device, train_loader, optimizer, epoch):\n",
    "    model.train()\n",
    "    for batch_idx, (data, target) in enumerate(train_loader):\n",
    "        data, target = data.to(device), target.to(device)\n",
    "        optimizer.zero_grad()\n",
    "        output = model(data)\n",
    "        loss = F.nll_loss(output, target)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        if batch_idx % args.log_interval == 0:\n",
    "            print('Train Epoch: {} [{}/{} ({:.0f}%)]\\tLoss: {:.6f}'.format(\n",
    "                epoch, batch_idx * len(data), len(train_loader.dataset),\n",
    "                100. * batch_idx / len(train_loader), loss.item()))\n",
    "\n",
    "def test(args, model, device, test_loader):\n",
    "    model.eval()\n",
    "    test_loss = 0\n",
    "    correct = 0\n",
    "    with torch.no_grad():\n",
    "        for data, target in test_loader:\n",
    "            data, target = data.to(device), target.to(device)\n",
    "            output = model(data)\n",
    "            test_loss += F.nll_loss(output, target, reduction='sum').item() # sum up batch loss\n",
    "            pred = output.max(1, keepdim=True)[1] # get the index of the max log-probability\n",
    "            correct += pred.eq(target.view_as(pred)).sum().item()\n",
    "\n",
    "    test_loss /= len(test_loader.dataset)\n",
    "    print('\\nTest set: Average loss: {:.4f}, Accuracy: {}/{} ({:.0f}%)\\n'.format(\n",
    "        test_loss, correct, len(test_loader.dataset),\n",
    "        100. * correct / len(test_loader.dataset)))\n",
    "    \n",
    "# Training settings\n",
    "parser = argparse.ArgumentParser(description='PyTorch MNIST Example')\n",
    "parser.add_argument('--batch-size', type=int, default=64, metavar='N',\n",
    "                    help='input batch size for training (default: 64)')\n",
    "parser.add_argument('--test-batch-size', type=int, default=1000, metavar='N',\n",
    "                    help='input batch size for testing (default: 1000)')\n",
    "parser.add_argument('--epochs', type=int, default=10, metavar='N',\n",
    "                    help='number of epochs to train (default: 10)')\n",
    "parser.add_argument('--lr', type=float, default=0.01, metavar='LR',\n",
    "                    help='learning rate (default: 0.01)')\n",
    "parser.add_argument('--momentum', type=float, default=0.5, metavar='M',\n",
    "                    help='SGD momentum (default: 0.5)')\n",
    "parser.add_argument('--no-cuda', action='store_true', default=False,\n",
    "                    help='disables CUDA training')\n",
    "parser.add_argument('--seed', type=int, default=1, metavar='S',\n",
    "                    help='random seed (default: 1)')\n",
    "parser.add_argument('--log-interval', type=int, default=10, metavar='N',\n",
    "                    help='how many batches to wait before logging training status')\n",
    "\n",
    "# Train this model with 60 epochs and after process every 300 batches log the train status \n",
    "args = parser.parse_args(['--epochs', '60', '--log-interval', '300'])\n",
    "\n",
    "use_cuda = not args.no_cuda and torch.cuda.is_available()\n",
    "\n",
    "torch.manual_seed(args.seed)\n",
    "\n",
    "device = torch.device(\"cuda\" if use_cuda else \"cpu\")\n",
    "\n",
    "kwargs = {'num_workers': 1, 'pin_memory': True} if use_cuda else {}\n",
    "train_loader = torch.utils.data.DataLoader(\n",
    "    datasets.MNIST('../data', train=True, download=True,\n",
    "                    transform=transforms.Compose([\n",
    "                        transforms.ToTensor(),\n",
    "                        transforms.Normalize((0.1307,), (0.3081,))\n",
    "                    ])),\n",
    "    batch_size=args.batch_size, shuffle=True, **kwargs)\n",
    "test_loader = torch.utils.data.DataLoader(\n",
    "    datasets.MNIST('../data', train=False, transform=transforms.Compose([\n",
    "                        transforms.ToTensor(),\n",
    "                        transforms.Normalize((0.1307,), (0.3081,))\n",
    "                    ])),\n",
    "    batch_size=args.test_batch_size, shuffle=True, **kwargs)\n",
    "\n",
    "\n",
    "model = Net().to(device)\n",
    "optimizer = optim.SGD(model.parameters(), lr=args.lr, momentum=args.momentum)\n",
    "\n",
    "for epoch in range(1, args.epochs + 1):\n",
    "    train(args, model, device, train_loader, optimizer, epoch)\n",
    "    test(args, model, device, test_loader)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Save the trained model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the trained model to a file\n",
    "torch.save(model.state_dict(), 'output/mnist.pth')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Export the trained model to ONNX \n",
    "In order to export the model, Pytorch exporter needs to run the model once and save this resulting traced model to a ONNX file. Therefore, we need to provide the input for our MNIST model. Here we would expect to get a black and white 28 x 28 picture as an input to run this model in inference phase."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.autograd import Variable\n",
    "\n",
    "# Load the trained model from file\n",
    "trained_model = Net()\n",
    "trained_model.load_state_dict(torch.load('output/mnist.pth'))\n",
    "\n",
    "# Export the trained model to ONNX\n",
    "dummy_input = Variable(torch.randn(1, 1, 28, 28)) # one black and white 28 x 28 picture will be the input to the model\n",
    "torch.onnx.export(trained_model, dummy_input, \"output/mnist.onnx\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "PS. You can examine the graph of this mnist.onnx file using an ONNX viewer call [Netron](https://github.com/lutzroeder/Netron)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import the ONNX model to Tensorflow\n",
    "We will use onnx_tf.backend.prepare to import the ONNX model into Tensorflow."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /home/hud/Documents/vision/4_pytorch-onnx-tf/output/onnx-tensorflow/onnx_tf/handlers/backend/ceil.py:10: The name tf.ceil is deprecated. Please use tf.math.ceil instead.\n",
      "\n",
      "WARNING:tensorflow:From /home/hud/Documents/vision/4_pytorch-onnx-tf/output/onnx-tensorflow/onnx_tf/handlers/backend/depth_to_space.py:12: The name tf.depth_to_space is deprecated. Please use tf.compat.v1.depth_to_space instead.\n",
      "\n",
      "WARNING:tensorflow:From /home/hud/Documents/vision/4_pytorch-onnx-tf/output/onnx-tensorflow/onnx_tf/handlers/backend/erf.py:9: The name tf.erf is deprecated. Please use tf.math.erf instead.\n",
      "\n",
      "WARNING:tensorflow:\n",
      "The TensorFlow contrib module will not be included in TensorFlow 2.0.\n",
      "For more information, please see:\n",
      "  * https://github.com/tensorflow/community/blob/master/rfcs/20180907-contrib-sunset.md\n",
      "  * https://github.com/tensorflow/addons\n",
      "  * https://github.com/tensorflow/io (for I/O related ops)\n",
      "If you depend on functionality not listed there, please file an issue.\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/hud/Documents/vision/4_pytorch-onnx-tf/output/onnx-tensorflow/onnx_tf/common/__init__.py:96: UserWarning: onnx_tf.common.get_outputs_names is deprecated. It will be removed in future release. Use TensorflowGraph.get_outputs_names instead.\n",
      "  warnings.warn(message)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /home/hud/Documents/vision/4_pytorch-onnx-tf/output/onnx-tensorflow/onnx_tf/handlers/backend/is_nan.py:9: The name tf.is_nan is deprecated. Please use tf.math.is_nan instead.\n",
      "\n",
      "WARNING:tensorflow:From /home/hud/Documents/vision/4_pytorch-onnx-tf/output/onnx-tensorflow/onnx_tf/handlers/backend/log.py:10: The name tf.log is deprecated. Please use tf.math.log instead.\n",
      "\n",
      "WARNING:tensorflow:From /home/hud/Documents/vision/4_pytorch-onnx-tf/output/onnx-tensorflow/onnx_tf/handlers/backend/random_normal.py:9: The name tf.random_normal is deprecated. Please use tf.random.normal instead.\n",
      "\n",
      "WARNING:tensorflow:From /home/hud/Documents/vision/4_pytorch-onnx-tf/output/onnx-tensorflow/onnx_tf/handlers/backend/random_uniform.py:9: The name tf.random_uniform is deprecated. Please use tf.random.uniform instead.\n",
      "\n",
      "WARNING:tensorflow:From /home/hud/Documents/vision/4_pytorch-onnx-tf/output/onnx-tensorflow/onnx_tf/handlers/backend/reciprocal.py:10: The name tf.reciprocal is deprecated. Please use tf.math.reciprocal instead.\n",
      "\n",
      "WARNING:tensorflow:From /home/hud/Documents/vision/4_pytorch-onnx-tf/output/onnx-tensorflow/onnx_tf/handlers/backend/space_to_depth.py:12: The name tf.space_to_depth is deprecated. Please use tf.compat.v1.space_to_depth instead.\n",
      "\n",
      "WARNING:tensorflow:From /home/hud/Documents/vision/4_pytorch-onnx-tf/output/onnx-tensorflow/onnx_tf/handlers/backend/upsample.py:16: The name tf.image.resize_images is deprecated. Please use tf.image.resize instead.\n",
      "\n",
      "WARNING:tensorflow:From /home/hud/Documents/vision/4_pytorch-onnx-tf/output/onnx-tensorflow/onnx_tf/handlers/backend/xor.py:10: The name tf.logical_xor is deprecated. Please use tf.math.logical_xor instead.\n",
      "\n",
      "WARNING:tensorflow:From /home/hud/Documents/vision/4_pytorch-onnx-tf/output/onnx-tensorflow/onnx_tf/backend.py:124: The name tf.placeholder is deprecated. Please use tf.compat.v1.placeholder instead.\n",
      "\n",
      "WARNING:tensorflow:From /home/hud/Documents/vision/4_pytorch-onnx-tf/output/onnx-tensorflow/onnx_tf/handlers/backend/dilated_pooling.py:640: The name tf.nn.max_pool_v2 is deprecated. Please use tf.nn.max_pool instead.\n",
      "\n",
      "WARNING:tensorflow:From /home/hud/Documents/vision/4_pytorch-onnx-tf/output/onnx-tensorflow/onnx_tf/handlers/backend/reshape.py:26: where (from tensorflow.python.ops.array_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use tf.where in 2.0, which has the same broadcast rule as np.where\n",
      "WARNING:tensorflow:From /home/hud/Documents/vision/4_pytorch-onnx-tf/output/onnx-tensorflow/onnx_tf/handlers/backend/reshape.py:31: sparse_to_dense (from tensorflow.python.ops.sparse_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Create a `tf.sparse.SparseTensor` and use `tf.sparse.to_dense` instead.\n",
      "WARNING:tensorflow:From /home/hud/Documents/vision/4_pytorch-onnx-tf/output/onnx-tensorflow/onnx_tf/handlers/backend/gemm.py:14: flatten (from tensorflow.python.layers.core) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use keras.layers.flatten instead.\n",
      "WARNING:tensorflow:From /home/hud/.virtualenvs/temp/lib/python3.6/site-packages/tensorflow_core/python/layers/core.py:332: Layer.apply (from tensorflow.python.keras.engine.base_layer) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use `layer.__call__` method instead.\n"
     ]
    }
   ],
   "source": [
    "import onnx\n",
    "from onnx_tf.backend import prepare\n",
    "\n",
    "# Load the ONNX file\n",
    "model = onnx.load('output/mnist.onnx')\n",
    "\n",
    "# Import the ONNX model to Tensorflow\n",
    "tf_rep = prepare(model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's explore the tf_rep object return from onnx.tf.backend.prepare"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "inputs: ['input.1']\n",
      "outputs: ['20']\n",
      "tensor_dict:\n",
      "{'conv1.bias': <tf.Tensor 'conv1.bias:0' shape=(10,) dtype=float32>, 'conv1.weight': <tf.Tensor 'conv1.weight:0' shape=(10, 1, 5, 5) dtype=float32>, 'conv2.bias': <tf.Tensor 'conv2.bias:0' shape=(20,) dtype=float32>, 'conv2.weight': <tf.Tensor 'conv2.weight:0' shape=(20, 10, 5, 5) dtype=float32>, 'fc1.bias': <tf.Tensor 'fc1.bias:0' shape=(50,) dtype=float32>, 'fc1.weight': <tf.Tensor 'fc1.weight:0' shape=(50, 320) dtype=float32>, 'fc2.bias': <tf.Tensor 'fc2.bias:0' shape=(10,) dtype=float32>, 'fc2.weight': <tf.Tensor 'fc2.weight:0' shape=(10, 50) dtype=float32>, 'input.1': <tf.Tensor 'input.1:0' shape=(1, 1, 28, 28) dtype=float32>, '9': <tf.Tensor 'transpose_2:0' shape=(1, 10, 24, 24) dtype=float32>, '10': <tf.Tensor 'transpose_4:0' shape=(1, 10, 12, 12) dtype=float32>, '11': <tf.Tensor 'Relu_2:0' shape=(1, 10, 12, 12) dtype=float32>, '12': <tf.Tensor 'transpose_7:0' shape=(1, 20, 8, 8) dtype=float32>, '13': <tf.Tensor 'transpose_9:0' shape=(1, 20, 4, 4) dtype=float32>, '14': <tf.Tensor 'Relu_5:0' shape=(1, 20, 4, 4) dtype=float32>, '15': <tf.Tensor 'Constant_6:0' shape=(2,) dtype=int64>, '16': <tf.Tensor 'Reshape_7:0' shape=(1, 320) dtype=float32>, '17': <tf.Tensor 'add_3:0' shape=(1, 50) dtype=float32>, '18': <tf.Tensor 'Relu_9:0' shape=(1, 50) dtype=float32>, '19': <tf.Tensor 'add_4:0' shape=(1, 10) dtype=float32>, '20': <tf.Tensor 'LogSoftmax_11:0' shape=(1, 10) dtype=float32>}\n"
     ]
    }
   ],
   "source": [
    "# Input nodes to the model\n",
    "print('inputs:', tf_rep.inputs)\n",
    "\n",
    "# Output nodes from the model\n",
    "print('outputs:', tf_rep.outputs)\n",
    "\n",
    "# All nodes in the model\n",
    "print('tensor_dict:')\n",
    "print(tf_rep.tensor_dict)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Run the model in Tensorflow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Image 1:\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAABwAAAAcCAAAAABXZoBIAAABVUlEQVR4nH2SPUhCURzFz3u+Ry8zkxSsJJegDxwagqygoaGWhoKWtvZwaoqm5miIQKhobpH2WhuCPl3CHHIqMssUlfdCeb7TkB9P5Xbu8uf87oE/514p66UEgZTGlI2mVai+0Xl/w5IaSf1Wd9AspJJb080ALZIkqz+Goev519hKgjU1YV2laKQ+yh1buBYrydrYCTE4/iCGrsCLGKLP+Af2KBUxlK2yGMK0xJD1thWbmU5UtJEgUCj2tsP88c2wu5zyRia/u9qTpc2BHZ/MwvX2QjlUN2vdVg9n36okmb+amThr65ZfqwEZADye3NTBZWvSOl/OkKQRW9p735+7J0k2Hjt3FF/v1p/v/Gvh/sJp/MQJ209g5uJJVYfGggEN+NzYDdshWMzD4XSrAMBHLdQCWws0tbaGbFIUUbfNKwQFiJKiCsMSlA/TBCQCEkBA4t8BIBu/r7nBmoPxX4YAAAAASUVORK5CYII=\n",
      "text/plain": [
       "<PIL.Image.Image image mode=L size=28x28 at 0x7F2203F94828>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /home/hud/Documents/vision/4_pytorch-onnx-tf/output/onnx-tensorflow/onnx_tf/backend_rep.py:63: The name tf.Session is deprecated. Please use tf.compat.v1.Session instead.\n",
      "\n",
      "WARNING:tensorflow:From /home/hud/Documents/vision/4_pytorch-onnx-tf/output/onnx-tensorflow/onnx_tf/backend_rep.py:81: The name tf.global_variables_initializer is deprecated. Please use tf.compat.v1.global_variables_initializer instead.\n",
      "\n",
      "The digit is classified as  2\n",
      "Image 2:\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAABwAAAAcCAAAAABXZoBIAAABm0lEQVR4nGWSQWsTURSFz70zUYRCLRVSaUlFIRUEbfMHhEpx50LqtsWtoHt37l1mkV/QLt2IILiI4rIQxGgpNJpYbCNtY2iraMO8e7p4ybwZvZs3Mx/ce+43TwgAMH8oBJkSAmD4RFPJQYrtHscUuzBdAOBSHAOg2FbzYqLcGR+bKZcjmGbaggMngB72f375WF6cG83xMFN79feVBxOeSjatiADorrefTnvKXJlzJNeeDWgkNd9UVC3Bvc4mCOAfCBhiHGJiuHVoaeackfzx6AVpJAO04dl7+XDN/Ftmld5BQSX52sDSfOTDxmFa++3xa9fAq7tRTvwwip7sSXfjXLF05bJ39J+h01bzW2f5jqf5tImR5IeVN2RIay44MrK5uk83MkRJZYhKguuVHaSG5M9vhOEKnUQKiU+1U7GQiv53q/dZfLeNFBKSJClUlJ5U+/EwlAG9jckUiuH2ped/IxEAotFR7cZVhD0d+4/vfx548/Xl6i9aRrzp0Xr92sIUtdWYurV0Pq+Pgt73louT+OZsAbkLhty1Hz2fAW38JkbPr/dOAAAAAElFTkSuQmCC\n",
      "text/plain": [
       "<PIL.Image.Image image mode=L size=28x28 at 0x7F2203F08EB8>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The digit is classified as  2\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from IPython.display import display\n",
    "from PIL import Image\n",
    "\n",
    "print('Image 1:')\n",
    "img = Image.open('assets/two.png').resize((28, 28)).convert('L')\n",
    "display(img)\n",
    "output = tf_rep.run(np.asarray(img, dtype=np.float32)[np.newaxis, np.newaxis, :, :])\n",
    "print('The digit is classified as ', np.argmax(output))\n",
    "\n",
    "print('Image 2:')\n",
    "img = Image.open('assets/three.png').resize((28, 28)).convert('L')\n",
    "display(img)\n",
    "output = tf_rep.run(np.asarray(img, dtype=np.float32)[np.newaxis, np.newaxis, :, :])\n",
    "print('The digit is classified as ', np.argmax(output))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Save the Tensorflow model into a file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf_rep.export_graph('output/mnist.pb')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "celltoolbar": "Raw Cell Format",
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
